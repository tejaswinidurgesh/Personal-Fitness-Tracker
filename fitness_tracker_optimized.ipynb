{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a6bad-c2a5-454b-829b-daa33bbe8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f12db5-618a-4c70-bb64-c13111d8427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59e2dc-f06f-4dfb-bc7e-9792a688cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f4d99-c42c-46d8-8ce7-6459b01a8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "calories = pd.read_csv(\"calories.csv\")\n",
    "exercise = pd.read_csv(\"exercise.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aedd30-0337-4e41-a3c3-38cf38fb8d3b",
   "metadata": {},
   "source": [
    "## General Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd6d95-d6be-42b5-9b63-9433ef987be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "calories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c3ec4-eb2e-4fd2-b208-f22671187f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f9440-bf44-4abf-8331-68b2291b77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df = exercise.merge(calories , on = \"User_ID\")\n",
    "exercise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70a3a4-6504-441b-85ed-8651aca3651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This dataset has \" , exercise_df.shape[0] ,\" instances and \",  exercise_df.shape[1] , \" columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afddcf-b174-4632-9db0-6d32985e7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns : \")\n",
    "for i , column in zip(range(len(exercise_df.columns)) , exercise_df.columns):\n",
    "  print(\"\\t\" , i + 1 , \".\" ,  column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821ffa2-171d-4f3f-80f7-b1d17a7c4e48",
   "metadata": {},
   "source": [
    "1.**User_ID** : The ID of the person which is unique.\\\n",
    "2.**Gender** : Gender of the person.\\\n",
    "3.**Age** : Age of the person.\\\n",
    "4.**Height** : Height of the person in $cm$.\\\n",
    "5.**Weight** : Weight of the person in $kg$.\\\n",
    "6.**Duration** : Duration of the person's exercise/activity.\\\n",
    "7.**Heart_Rate** : Heart rate per $min$ of the person.\\\n",
    "8.**Body_Temp** : Body temperature of the person in $C^{\\circ}$.\\\n",
    "9.**Calories** : Calories burned in kilo calories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d5767-c796-47df-9778-03abd5f0b9ac",
   "metadata": {},
   "source": [
    "### Dataset's Overall Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ff324-e29d-44bb-8b9c-b8f00fffa3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e90a3-60d0-4919-9353-2cc22e347abf",
   "metadata": {},
   "source": [
    "* As we can see, the table above shows the `Descriptive Statistics`(for example `centeral tendency`) of each column or feature.\n",
    "\n",
    "* For example for `Age` column.%25 of the data lie between **20** and **28**, anohter %25 lie between **28** and **39**, and so on.The box plot shows the exact concept that I just mentioned.\n",
    "\n",
    "* The outliers are shown with dots in box plots, which we will discuss about them in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac7490-e99d-47b4-a1d0-94a3b79094a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['b' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'w' , 'b']\n",
    "fig1 , axes = plt.subplots(len(exercise_df.columns) , 1 , figsize = (10 , 20))\n",
    "plt.subplots_adjust(wspace = 0.3 , hspace = 0.7)\n",
    "axes = axes.flatten()             #for using axes indeces with one dimention array instead of two dimension\n",
    "for i , column in zip(range(len(exercise_df.columns)) , exercise_df.columns):\n",
    "  try:\n",
    "    sns.boxplot(data = exercise_df , x = column , color = c[i] , ax = axes[i])\n",
    "  except:\n",
    "    fig1.delaxes(axes[i])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee1f2c-346d-47f3-b3cd-bf7a1f6669d2",
   "metadata": {},
   "source": [
    "### Overall information of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c90a0-4f54-4ba0-b1a5-5a992ca047ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a8ea4-3239-4641-a464-f8c983a0fb0e",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "In this section we are going to check whether this dataset has null values or not.We will check this with heatmap.Because it is easy to understand and we can see dataset's condition at a glance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b5ba9-c620-4167-840d-6b9258569465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(exercise_df.isnull() , yticklabels = False , cbar = False , cmap = \"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1d0e8-b546-44d0-9e60-36184f4bc46f",
   "metadata": {},
   "source": [
    "* As we can see, fortunately, this dataset does not have any null/NaN values, which is good and it is not necessary to do extra manipulations(for instance imputation , dropping or filling NaN values etc.) with this dataset.\n",
    "\n",
    "### Drop Duplicates\n",
    "Lets assure that this dataset does not contain any duplicate values in `User_ID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49145a9-9690-44af-b6d8-e60c7a208d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of dataset before dropping duplicates : \" , exercise_df.shape)\n",
    "exercise_df.drop_duplicates(subset = ['User_ID'], keep='last' , inplace = True)    # Keeping the first example of duplicates in 'User_ID' column.\n",
    "print(\"The shape of dataset after dropping duplicates : \" , exercise_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d5194-daab-4366-be01-603427e7781d",
   "metadata": {},
   "source": [
    "* As we can see the shape of dataset before and after dropping duplicates is the same.It is a good sign, because we do not need to be worry about `Data Leakage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcf591-6518-4b0c-9043-8b4034e1f815",
   "metadata": {},
   "source": [
    "* In the next step we have to delete `User_ID` feature.Because it is a `low predictive feature`.In other words, it is not only a useless feature for our calorie burned prediction model but also has a negative impact on model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f8c9-dd18-4a1a-98a8-1bbe158753bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df.drop(columns = \"User_ID\" , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27159336-e13d-45e2-9153-e0d8ed22265e",
   "metadata": {},
   "source": [
    "* For avoiding any `Data Leakage` in our model, let's split our data into training set and test set before doing any `feature engineering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65edc1ed-d202-41ae-9e43-5c1a981b9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_train_data , exercise_test_data = train_test_split(exercise_df , test_size = 0.2 , random_state = 1)\n",
    "print(\"Shape of training data : \" , exercise_train_data.shape)\n",
    "print(\"Shape of test data : \" , exercise_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873f62e-675d-4112-9697-5e978435835e",
   "metadata": {},
   "source": [
    "### Dataset's Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396674b-b3e7-4593-b31c-1b71455633b8",
   "metadata": {},
   "source": [
    "One of the main criterions that whether we will be able to deploy our model into production or not, is that **the distribution of features for both training set and test set must be similar**.This is because the model is fitting on the training set and the model keeps in mind the training set patterns.When the distribution of test set is different from the training set it means that the model can not predict very well on test set examples and unlike the training set accuracy, the testing set accuracy will be low.This is because, at first, we have to see and compare the distributions for both test set and training set and check whether both have the same distribution or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bc203-e46d-4d6a-a424-ee9ebea4e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada624f-7bc9-4bf7-8a77-ec5d24458ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['b' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'w' , 'b']\n",
    "fig1 , axes = plt.subplots(len(exercise_train_data.columns) , 2 , figsize = (10 , 20))\n",
    "plt.subplots_adjust(wspace = 0.3 , hspace = 0.7)\n",
    "axes = axes.flatten()             #for using axes indeces with one dimention array instead of two dimension\n",
    "\n",
    "for i , column , color in zip(range(0 , len(exercise_train_data.columns) * 2 , 2) , exercise_train_data.columns , c):\n",
    "  try:\n",
    "    axes[i].title.set_text(column + \" Train Distribution\")\n",
    "    sns.kdeplot(data = exercise_train_data , x = column , ax = axes[i] , color = color)\n",
    "  except:\n",
    "    fig1.delaxes(axes[i])\n",
    "    continue\n",
    "\n",
    "for i , column , color in zip(range(1 , len(exercise_train_data.columns) * 2 , 2) , exercise_train_data.columns , c):\n",
    "  try:\n",
    "    axes[i].title.set_text(column + \" Test Distribution\")\n",
    "    sns.kdeplot(data = exercise_test_data , x = column , ax = axes[i] , color = color)\n",
    "  except:\n",
    "    fig1.delaxes(axes[i])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981a3c2-d933-4ec3-aac3-7bb9f48a9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(exercise_train_data[[\"Weight\" , \"Height\" , \"Duration\" , \"Heart_Rate\" , \"Calories\" , \"Gender\"]] , hue = \"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd640a-c95e-4934-b477-9acfe015c8d5",
   "metadata": {},
   "source": [
    "* As we can see from graphs above, there is not a specific correlation or relationship between most of the features in the dataset.For example,there is not a specific relationship between `Duration` and `Weight` or between `Duration` and `Hight`.This is because exercisers may have different exercise duration no matter of their `Weight` and `Height`.\n",
    "\n",
    "* In some cases, a featrue has a low relationship with another feature, like `Duration` and `Heart_Rate`.Somehow(with low confident) we can say that the more time somebody exercises the more 'Heart Rate' per minute he/she will have.\n",
    "\n",
    "* In some cases , two featrues have a high relationship(in compare to last two cases), like `Height` and `Weight`.\n",
    "---\n",
    "* There are more informations and benefits that we can get from `Correlation` concept.But thats for now and we will go further in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2e133-1e3f-4410-98ac-dc5a7cf22180",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8348f-14f1-4737-b611-1e927a13469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum age in dataset is : ' , exercise_train_data[\"Age\"].min())\n",
    "print('Maximum age in dataset is : ' , exercise_train_data[\"Age\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423f95a-bda9-4448-9274-e57e628cfe15",
   "metadata": {},
   "source": [
    "* As we can see the oldest person in dataset is 79 years old and the youngest is 20 years old.What we want to do is divide this range of ages into several named ranges.In other words we want to convert the continuous column into categorical column.\\\n",
    "The ranges are:\n",
    "\n",
    "| Name            | Age           |\n",
    "| ----------------|:-------------:|\n",
    "| **Young**       |[20 , 40)      |\n",
    "| **Middle-Aged** |[40 , 60)      |\n",
    "| **Old**         |[60 , 80)      |\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52f696-447c-49c1-965c-484ca60d7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_groups = [\"Young\" , \"Middle-Aged\" , \"Old\"]\n",
    "exercise_train_data[\"age_groups\"] = pd.cut(exercise_train_data[\"Age\"] , bins = [20 , 40 ,60 , 80] , right = False , labels = age_groups)\n",
    "exercise_train_data[\"age_groups\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22e622-5f0e-48cb-a759-cf3599234c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_train_data[\"age_groups\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe0560-1a84-4773-bdb4-7027e1614317",
   "metadata": {},
   "source": [
    "* As we can see we have just converted a continuous column into a categorical column.Now its time to analyze `age_groups` column in terms of different aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf7f48-8ccd-4dee-b5dd-b1ab701df28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 8 , 6\n",
    "sns.countplot(data = exercise_train_data , x = \"age_groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7536ec-a2ac-4d72-85ed-562e82e58ff1",
   "metadata": {},
   "source": [
    "* As we expected, there is a significant difference between in counts of different age groups.Most of the people of this dataset are **young**.The second is **middle-aged** and the third one is **old**.\n",
    "\n",
    "Lets analyze how many kilocalories each age groups burned.We will do this with `box plot`.Because `box plot` has a intuitive graph that we can extract **Median** , **Interquartile Range** , **Outliers** and etc.Just like the picture below shows.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/18000/1*2c21SkzJMf3frPXPAR_gZA.png \"Box plot structure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d10b78-7e10-465c-ba2f-dc1145eec76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(exercise_train_data , x= \"age_groups\" , y = \"Calories\" , color = \"Gender\")\n",
    "\n",
    "fig.update_layout(      \n",
    "    width=700,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443ea46-b9b0-4d2c-8ccc-a15848309a66",
   "metadata": {},
   "source": [
    "* As we can observe, old individuals have burned more kilocalories in compare of two other age groups.And the young persons are the least in burning kilocalories which is a surprise!\n",
    "\n",
    "* Another interesting thing is, females in all age ranges performed very similar.In other words, they burned same amount of kilocalories in average.But for males, old group outperformed and the youth have the weakest performance.\n",
    "\n",
    "* Also there is an outlier for young group which is shown by a point.This point has a value which is greater than third quartile value(Q3) plus 1.5 times of interquartile range magnitude.\n",
    "\\\n",
    "\\\n",
    "`Outlier` > `Q3 + 1.5 * IQR`\\\n",
    "OR\\\n",
    "`Outlier` < `Q1 - 1.5 * IQR`\n",
    "\n",
    "\n",
    "Now lets see which group have the most exercise duration in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268f92c-b68e-40ca-bd2b-9ad79c926e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(exercise_train_data , x= \"age_groups\" , y = \"Duration\" , color = \"Gender\")\n",
    "\n",
    "fig.update_layout(      \n",
    "    width=750,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74280d1-c266-4cf6-80b8-c51c98bc6643",
   "metadata": {},
   "source": [
    "* As we can see , the exercise duration of each group is pretty identical.Every group have the same interquartile range , median and so on.\n",
    "\n",
    "* In addition, the duration is very similar for males and females in `old` and `middle-aged` groups.But in `youth`, males outperformed.\n",
    "\n",
    "* Another tip is, the median exercise duration of this dataset is about 15 minutes.We will assure this by the code bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedceb7-6588-4237-8a3f-1174a9215159",
   "metadata": {},
   "source": [
    "### Gender\n",
    "Lets plot the count plot of each gender to see how many exercisers are male and how many of them are female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f7174-fcf8-4d18-9958-76460c7eb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 8 , 6\n",
    "sns.countplot(data = exercise_train_data , x = \"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514dc04-b1f2-4d3f-9e0d-1957fa2b3e77",
   "metadata": {},
   "source": [
    "* As we can see, number of females are slightly higher than man but this distinction is not significant.we can say, in general, they are equal.\n",
    "\n",
    "\n",
    "\\\n",
    "In this section, lets compare the exercise duration between males and females. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac87460-5867-40f5-a36f-e9a59734cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(exercise_train_data , x= \"Gender\" , y = \"Duration\")\n",
    "\n",
    "fig.update_layout(      \n",
    "    width=700,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e741742-e635-448a-8e4a-28d8614d5096",
   "metadata": {},
   "source": [
    "* As we can observe the median , IQR and etc. for both groups are overly identical.We proved this fact in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c14a95-0aaa-4d4b-8dbd-5bc82994a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(exercise_train_data , x= \"Gender\" , y = \"Heart_Rate\")\n",
    "\n",
    "fig.update_layout(      \n",
    "    width=700,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d750a-0955-484d-8b67-de1f5e18cdb5",
   "metadata": {},
   "source": [
    "* As we can see, again the overall heart rate of both male and female are similar.In addition to this, we have an outliers for the male."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6f189-a0a4-4e1b-9d1e-9f2a2a25dfa8",
   "metadata": {},
   "source": [
    "\n",
    "In this section our purpose is combine the `Weight` column and the `Height` column values to perform a simple BMI calculation to classify individuals of this dataset into different groups according to their BMI value.\n",
    "\n",
    "* The BMI(Body Mass Index) formula:\n",
    "\n",
    "\n",
    "$BMI = \\frac{Weight(kg)}{Height(m)^2}$\n",
    "\n",
    "OR\n",
    "\n",
    "$BMI = \\frac{Weight(Ib)}{Height(in)^2}$\n",
    "\n",
    "\n",
    "* The first formula will be used because the units for `Weight` and `Height` of this dataset is `kg` and `meter` in respect.\n",
    "\n",
    "* According to [this page](https://en.wikipedia.org/wiki/Body_mass_index) we will classify instances according to below table:\n",
    "\n",
    "\n",
    "|   Categoty                                       | from          | to    |\n",
    "| -------------------------------------------------|:-------------:| -----:|\n",
    "| Very severely underweight                        | --            |  15   |\n",
    "| Severely underweight                             | 15            |  16   |\n",
    "| Underweight                                      | 16            |  18.5 |\n",
    "| Normal (healthy weight)                          | 18.5          |  25   |\n",
    "| Overweight                                       | 25            |  30   |\n",
    "| Obese Class I (Moderately obese)                 | 30            |  35   |\n",
    "| Obese Class II (Severely obese)                  | 35            |  40   |\n",
    "| Obese Class III (Very severely obese)            | 40            |       |\n",
    "\n",
    "* We will classify examples according to above category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073b7b2-57fb-4bba-8d0f-3300f0020ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [exercise_train_data , exercise_test_data]:         # adding BMI column to both training and test sets\n",
    "  data[\"BMI\"] = data[\"Weight\"] / ((data[\"Height\"] / 100) ** 2)\n",
    "  data[\"BMI\"] = round(data[\"BMI\"] , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb8f67-8fcc-42dc-8105-7411f2f20c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise_test_data[\"BMI\"] = exercise_test_data[\"Weight\"] / ((exercise_test_data[\"Height\"] / 100) ** 2)\n",
    "# exercise_test_data[\"BMI\"] = round(exercise_test_data[\"BMI\"] , 2)\n",
    "# exercise_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d288805-3144-4586-a700-485900e3035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_category = [\"Very severely underweight\" , \"Severely underweight\" ,\n",
    "                \"Underweight\" , \"Normal\" ,\n",
    "                \"Overweight\" , \"Obese Class I\" ,\n",
    "                \"Obese Class II\" , \"Obese Class III\"]\n",
    "\n",
    "exercise_train_data[\"Categorized_BMI\"] = pd.cut(exercise_train_data[\"BMI\"] , bins = [0 , 15 , 16 , 18.5 , 25 , 30 , 35 , 40 , 50]\n",
    "                                              , right = False , labels = bmi_category)\n",
    "\n",
    "exercise_train_data[\"Categorized_BMI\"] = exercise_train_data[\"Categorized_BMI\"].astype(\"object\") # converting 'categorical' dtype intp 'object' dtype for \"Categorized_BMI\" column\n",
    "exercise_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3b9f5-571f-4ff8-ac39-7537bad05e74",
   "metadata": {},
   "source": [
    "Now lets see the `Categorized_BMI` distribution in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a27790-98d2-409b-b6e0-c81d81ae4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = exercise_train_data[\"Categorized_BMI\"].value_counts().reset_index()\n",
    "ds.columns = [\"Categorized_BMI\" , \"Count\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd33b-ec10-442f-a876-762b9a3682b8",
   "metadata": {},
   "source": [
    "* As we can see, many parts of this dataset consists of `Normal` individuals.The second and last group is `Overweight` group.Other groups are not in the dataset which is normal.Because `Obese` and `Underweight` persons do not tend to do exercise.\n",
    "\n",
    "* So lets plot the boxplot of the first two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd91e75-066b-4b0b-933e-3dd277f5bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[(ds[\"Categorized_BMI\"] == \"Normal\") | (ds[\"Categorized_BMI\"] == \"Overweight\")]\n",
    "#ds[\"Categorized_BMI\"] = ds[\"Categorized_BMI\"].astype(\"object\")      \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 8 , 6\n",
    "sns.barplot(data = ds , x = \"Categorized_BMI\" , y = \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbfcbf-b017-4eef-b843-b7d101102f6d",
   "metadata": {},
   "source": [
    "* As we can see, many parts of this dataset consists of `Normal` individuals.The second and last group is `Overweight` group.Other groups are not in the dataset which is normal.Because `Obese` and `Underweight` persons do not tend to do exercise.\n",
    "\n",
    "Lets get into details and see how many of each group are male and how many of them are female.\n",
    "\n",
    "First of all lets prepare the dataset that shows the distributions of `Categorized_BMI` for each gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fbc69-6f3e-4b73-a3f3-4ad030699ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds =exercise_train_data[[\"Gender\" ,\"Categorized_BMI\"]].value_counts().reset_index().sort_values(by = [\"Gender\" , \"Categorized_BMI\"])\n",
    "ds.columns = [\"Gender\" , \"Categorized_BMI\" , \"Count\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62dbf84-8821-476b-a8e5-81c797aad4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 8 , 6\n",
    "sns.barplot(data = ds , x = \"Categorized_BMI\" , y = \"Count\" , hue = \"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abf301-73ea-4d9c-bd73-2e932b0aab21",
   "metadata": {},
   "source": [
    "* An interesting thing that this graph shows is the `weight` distribution between each gender.the number of males who are overweight is way more than the number of females that have the similar situation.And the number of females that are in their ideal weight is really large in compare of other categories.In general, women have a better situation than men.\n",
    "\n",
    "Now lets plot the `Categirized_BMI` distribution for each group ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac61c77-73a9-41b1-aa87-2c58f1279c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds =exercise_train_data[[\"age_groups\" ,\"Categorized_BMI\"]].value_counts().reset_index().sort_values(by = [\"age_groups\" , \"Categorized_BMI\"])\n",
    "ds.columns = [\"age_groups\" , \"Categorized_BMI\" , \"Count\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d80f97-0317-432c-b5d6-5e4676deab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 8 , 6\n",
    "sns.barplot(data = ds , x = \"Categorized_BMI\" , y = \"Count\" , hue = \"age_groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2435b4-2755-4345-9887-5c0541e43663",
   "metadata": {},
   "source": [
    "* As we can see the `Categorized_BMI` is identically distributed between age groups(the sequence is identical, for example in both `Normal` and `Overweight` ; `Young` comes first , 'Middle-Aged` comes second , etc.)\n",
    "\n",
    "* An interesting thing is , about `50%` of old individuals have `Normal` weight and another `50%` are `Overweight`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd8f25-9de0-4dd6-830e-f30b9280b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(exercise_train_data , x = \"Duration\" , y = \"Body_Temp\" , size = \"Calories\")\n",
    "\n",
    "fig.update_layout(      \n",
    "    width=700,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53dc272-25bc-4874-afd0-27e642667b99",
   "metadata": {},
   "source": [
    "### [Pearson] Corelation\n",
    "In this section we are going to analyze the correlations between each two features.The corrlation helps us to see , how much this two features' relationship is strong.If the the relationship of this two features is extremely strong(In other words the correlation is equall to 1 or -1 or close to this two numbers) we will face `collinearity` problem in our model.This means that not only this two feature will not help us to build a better model but also they cause some problems for our model.One way is to remove one of these feaures.\n",
    "\n",
    "\n",
    "* We use pearson correlation method:\n",
    "\n",
    "\n",
    "\n",
    "###### Covariance : **COV(X , Y)** = $$( \\frac{\\sum (X_{i} - \\bar{X})(Y_{i} - \\bar{Y})}{N}  )$$\n",
    "\n",
    "\n",
    "###### Correlation : **CORR(X , Y)** = $$( \\frac {COV(X , Y)}{\\sigma_{X} \\sigma_{Y}}  )$$\n",
    "\n",
    "**Where** :\n",
    "\n",
    "> 1. **Xi** : the values of the X-variable\n",
    "\n",
    "> 2. **Yj** :  the values of the Y-variable\n",
    "\n",
    "> 3. **X̄**  : the mean (average) of the X-variable\n",
    "\n",
    "> 4. **Ȳ**  : the mean (average) of the Y-variable\n",
    "\n",
    "> 5. **N**  : the number of data points\n",
    "\n",
    "> 6. **σX** : the standard deviation of the X-variable\n",
    "\n",
    "> 7. **σY** : the standard deviation of the Y-variable\n",
    "\n",
    "\n",
    "So now lets plot the heatmap to analyze the correlation of features in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d259393-bb9a-4ace-b7ec-d0a266cb5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc8239-4f28-4d78-b5b8-1d51f99c3f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad40250-3d6f-45cf-9e7c-49bb136b05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 8 , 6\n",
    "corr = exercise_train_data.corr(numeric_only = True)\n",
    "sns.heatmap(corr , annot = True , square = True , linewidth = .5 , vmin = 0 , vmax = 1 , cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f3132-2fac-493e-ac73-e68841e1e25d",
   "metadata": {},
   "source": [
    "* This heatmap shows the correlation of both features in each cell.As we can see, many features have high correlation with another feature.One thing that has to be mentioned is that we have to drop useless features as many as possible.Because when we have many features the dimension of fearture space will be very large and when our model runs on this features it will be very slow.Because of that we have to drop some features.\n",
    "* If two or more features have a high correlation with each other, we have to save one of them and drop the rest.In This way, we can improve model's efficiency.\n",
    "* According to the heatmap, `Weight` and `Height` have a high correlation but we combined them and put them into the `BMI` column.So we can drop `Weight` and `Height` columns and save `BMI`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5673b0-44f3-4681-8de2-b05fe4a10fdb",
   "metadata": {},
   "source": [
    "Before we feed our data to the model we have to first convert `categorical` column(like `Gender`) into `numerical` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de046d89-637f-4120-8dc0-6f49770343d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_train_data = exercise_train_data[[\"Gender\" , \"Age\" , \"BMI\" , \"Duration\" , \"Heart_Rate\" , \"Body_Temp\" , \"Calories\"]]\n",
    "exercise_test_data = exercise_test_data[[\"Gender\" , \"Age\" , \"BMI\"  , \"Duration\" , \"Heart_Rate\" , \"Body_Temp\" , \"Calories\"]]\n",
    "exercise_train_data = pd.get_dummies(exercise_train_data, drop_first = True)\n",
    "exercise_test_data = pd.get_dummies(exercise_test_data, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb723a-f9f3-45a2-95d9-acaafee608f0",
   "metadata": {},
   "source": [
    "* So now let's seperate X and y for training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadac45c-0623-4b99-884d-c24c2a18ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = exercise_train_data.drop(\"Calories\" , axis = 1)\n",
    "y_train = exercise_train_data[\"Calories\"]\n",
    "\n",
    "X_test = exercise_test_data.drop(\"Calories\" , axis = 1)\n",
    "y_test = exercise_test_data[\"Calories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93cb00-902c-4b25-a6e0-96820d0747c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086667b-e3c3-4492-9eae-f722f899b3b6",
   "metadata": {},
   "source": [
    "### Learning Curve\n",
    "Learning Curve is a plot of the model's performance on the training set and the validation set as a function of the training set size(or the training iteration).One of the concepts that we get is the appropriate number of examples in training set size.If we take a look at the plot down below we will see that both training set size and validation set reached a plateau at a certain training set size(for example `800` training set size).It means that, with only `800` training set size we will get similar results with our model in compare to `1000` , `2500` or `5000` training set size.In other word increasing the training set size more than `800` will not improve the model's performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce42ce-bf46-42d0-a49c-6557ce7ae902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors , val_errors = [] , []\n",
    "def plot_learning_curve(model):\n",
    "  for m in range(1 , 1000):\n",
    "    model.fit(X_train[:m] , y_train[:m])\n",
    "    y_train_predict = model.predict(X_train[:m])\n",
    "    y_val_predict = model.predict(X_test[:m])\n",
    "    train_errors.append(mean_squared_error(y_train[:m] , y_train_predict))\n",
    "    val_errors.append(mean_squared_error(y_test[:m] , y_val_predict))\n",
    "\n",
    "  plt.plot(np.sqrt(train_errors) , \"r-+\" , linewidth = 2 , label = \"Train\")\n",
    "  plt.plot(np.sqrt(val_errors) , \"b-\" , linewidth = 3 , label = \"Val\")\n",
    "  plt.title(\"Learning Curve\")\n",
    "  plt.xlabel(\"Training Set Size\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "  plt.xlim([0 , 1000])\n",
    "  plt.ylim([0 , 25])\n",
    "  plt.legend()\n",
    "linreg = LinearRegression()\n",
    "plot_learning_curve(linreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94dbab-7f95-467d-851c-3dd9e53d19da",
   "metadata": {},
   "source": [
    "* As we can see , both curves plateaued at `800` in training set size axis.So it means if we increase the training set size more than `800` examples, model's performance will not be improved significantly.So we can reduce the training set size up to `800` examples without decreasing the model's performance.\n",
    "\n",
    "* Another reason that the training set and validation set could not reach a lower MSE with more examples is that the dataset do not have enough informative features that our learning algorithm can leverage to build more performant model. \n",
    "\n",
    "### Building Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bed31-8231-4a2e-911c-d0d5ae767467",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train , y_train)\n",
    "linreg_prediction = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a87ee-661b-4986-91d7-d91f02e2791d",
   "metadata": {},
   "source": [
    "## Regression Evaluation Metrics\n",
    "\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "[Source](https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57951a2-2ad4-4f57-830d-523e5fc50b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression Mean Absolute Error(MAE) : \" , round(metrics.mean_absolute_error(y_test , linreg_prediction) , 2))\n",
    "print(\"Linear Regression Mean Squared Error(MSE) : \" , round(metrics.mean_squared_error(y_test , linreg_prediction) , 2))\n",
    "print(\"Linear Regression Root Mean Squared Error(RMSE) : \" , round(np.sqrt(metrics.mean_squared_error(y_test , linreg_prediction)) , 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760f334-e068-4d8a-919b-ebeecf8c1cf6",
   "metadata": {},
   "source": [
    "* So we have just implemented a simple `Linear Regression` to predict the calories burned with various parameters.Our RMSE for Linear Regression is about `12` which is acceptable.As just I said it is a simple model.We can reduce this errors with just replacing simple model with more complex model.\n",
    "\n",
    "* In the next section I will be using the `RandomForestRegressor` algorithm which uses `bagging` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e5899-4374-4ae6-9ed9-3f44c2284f58",
   "metadata": {},
   "source": [
    "### RandomForestRegressor Algorithm\n",
    "* In this section I will be using the `GridSearchCV` on of the cross-validation methods that we use for selecting `hyperparameters`.\n",
    "\n",
    "* In this section , I will be using 3 hyperparameters for RandomForestRegressor algorithm which is `n_estimators` , `max_features` and `max_depth`.This will run on 5 splits and the split with highest accuracy will be picked.\n",
    "\n",
    "**NOTE:** Sometimes running `GridSearchCV` is very time consuming in terms of `computational complexity`.So I have already run the GridSearchCV and put the best parameters into `RandomForestRegressor` directly and then run the model.So I commented out `GridSearchCV` and all of its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303def0d-0531-4603-9662-9acee00f5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest_reg = RandomForestRegressor()\n",
    "# param_grid = {'n_estimators' : [10 , 100 , 1000] , \n",
    "#               'max_features' : [2 , 2 , 3] , \n",
    "#               'max_depth' : [2 , 4 , 6]}\n",
    "\n",
    "# grid_search = GridSearchCV(forest_reg , param_grid , cv = 5 , scoring = 'neg_mean_squared_error' , n_jobs = -1 , verbose = 3)\n",
    "# grid_search.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3089f-9b11-4df1-bafd-26ac60a5fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709160d-f84c-4078-ad42-40d279c2f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dab32-876f-485a-9a2d-53768a829a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19f50f-a7e8-44f3-a4a5-af6e36435cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_reg = RandomForestRegressor(n_estimators = 1000 , max_features = 3 , max_depth = 6)\n",
    "random_reg.fit(X_train , y_train)\n",
    "random_reg_prediction = random_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba7e72-1439-456b-a011-b2c00fdc6091",
   "metadata": {},
   "source": [
    "* As you can see, we get a slightly better result with `RandomForestRegressor` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706ee1c-e149-47b8-a4d5-132f36d8523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RandomForest Mean Absolute Error(MAE) : \" , round(metrics.mean_absolute_error(y_test , random_reg_prediction) , 2))\n",
    "print(\"RandomForest Mean Squared Error(MSE) : \" , round(metrics.mean_squared_error(y_test , random_reg_prediction) , 2))\n",
    "print(\"RandomForest Root Mean Squared Error(RMSE) : \" , round(np.sqrt(metrics.mean_squared_error(y_test , random_reg_prediction)) , 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d688fd1-1850-4d72-a5d6-260ab0bed1fb",
   "metadata": {},
   "source": [
    "* As we can see the RMSE for `ReandomForestRegressor` is lower than `Linear Regresssion`'s RMSE.It means that we can make better predictions with RandomForestRegressor.\n",
    "\n",
    "And now let's make a prediction to test model's performance.First we selected an example randomly and passed some numbers (close to feature values of the example) to the model and compared the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea226c-e34b-498b-aea4-23b0b621ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_train_data.iloc[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba2928-a320-49ea-ac98-fa72fcc28eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.array([[24 , 25 , 28 , 100 , 40 , 1]]).reshape(1 , -1)\n",
    "y_pred = random_reg.predict(X_array)\n",
    "print(\"Prediction : \" , round(y_pred[0] , 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a20e0a-9d24-40ac-8337-24a2e6ff62b0",
   "metadata": {},
   "source": [
    "* As we can see this model did a good job and the predicted value is close to the real value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be02d6-a43f-49a0-a6cd-fb45818beb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
